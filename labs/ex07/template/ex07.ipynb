{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "## Classification Using SVM\n",
    "Load dataset. We will use a toy dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(N, D) = (569, 31)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "sklearn_dataset = datasets.load_breast_cancer()\n",
    "Xx  = sklearn_dataset.data\n",
    "y = sklearn_dataset.target * 2 - 1    # labels must be in {-1, 1} for the hinge loss\n",
    "X = np.ones((Xx.shape[0], Xx.shape[1] + 1 ))    # add a column of ones for intercept\n",
    "X[:, :-1] = Xx\n",
    "print(\"(N, D) =\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare cost and prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_primal_objective(y, X, w, lambda_):\n",
    "    \"\"\"compute the full cost (the primal objective), that is loss plus regularizer.\n",
    "    X: the full dataset matrix, shape = (num_examples, num_features)\n",
    "    y: the corresponding +1 or -1 labels, shape = (num_examples)\n",
    "    w: shape = (num_features)\n",
    "    \"\"\"\n",
    "    hinge = 1 - y * (X @ w)\n",
    "    hinge[hinge < 0] = 0\n",
    "    \n",
    "    reg = lambda_ / 2 * np.linalg.norm(w, 2)\n",
    "    \n",
    "    return hinge.sum() + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y, X, w):\n",
    "    \"\"\"compute the training accuracy on the training set (can be called for test set as well).\n",
    "    X: the full dataset matrix, shape = (num_examples, num_features)\n",
    "    y: the corresponding +1 or -1 labels, shape = (num_examples)\n",
    "    w: shape = (num_features)\n",
    "    \"\"\"\n",
    "    predictions = X @ w\n",
    "    predictions = np.sign(predictions)\n",
    "    \n",
    "    #removing unlikey 0\n",
    "    predictions[predictions == 0] = 1\n",
    "    \n",
    "    #accuracy is numer of correct classifications over total classifications\n",
    "    accuracy = 1 - (np.abs(y - predictions) / 2)\n",
    "    return accuracy.sum() / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the (stochastic) subgradient for the n-th summand of the SVM optimization objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stochastic_gradient(y, X, w, lambda_, n, num_examples):\n",
    "    \"\"\"compute the stochastic gradient of loss plus regularizer.\n",
    "    X: the dataset matrix, shape = (num_examples, num_features)\n",
    "    y: the corresponding +1 or -1 labels, shape = (num_examples)\n",
    "    w: shape = (num_features)\n",
    "    n: the index of the (one) datapoint we have sampled\n",
    "    num_examples: N\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    # Be careful about the constant N (size) term!\n",
    "    # The complete objective for SVM is a sum, not an average as in earlier SGD examples!\n",
    "    x_n, y_n = X[n], y[n]\n",
    "    \n",
    "    gradient_reg = lambda_ * w\n",
    "    hinge = 1 - y_n * x_n * w\n",
    "    gradient_hinge = -y_n * x_n\n",
    "    gradient_hinge[hinge < 0] = 0\n",
    "    return gradient_hinge + gradient_reg / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement stochastic gradient descent: Pick a data point uniformly at random and update w based on the gradient for the n-th summand of the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0, cost=42679.7148487474\n",
      "iteration=10000, cost=435.91510975297797\n",
      "iteration=20000, cost=445.5705533611234\n",
      "iteration=30000, cost=442.4525066941727\n",
      "iteration=40000, cost=447.14880468759225\n",
      "iteration=50000, cost=441.4201114487252\n",
      "iteration=60000, cost=441.5601205043288\n",
      "iteration=70000, cost=444.5811191324051\n",
      "iteration=80000, cost=442.0671074993543\n",
      "iteration=90000, cost=445.1708324672238\n",
      "iteration=100000, cost=443.274536293614\n",
      "iteration=110000, cost=438.7468721089962\n",
      "iteration=120000, cost=437.52522365687616\n",
      "iteration=130000, cost=437.83580592158535\n",
      "iteration=140000, cost=440.89415887951066\n",
      "iteration=150000, cost=437.29109870745083\n",
      "iteration=160000, cost=437.8683716068875\n",
      "iteration=170000, cost=438.4867354273954\n",
      "iteration=180000, cost=439.43814938034353\n",
      "iteration=190000, cost=440.81132461498106\n",
      "training accuracy = 0.6836555360281195\n",
      "[ 3.44333161e-03  4.93682269e-03  1.05796541e-02 -7.52361866e-04\n",
      "  3.57473220e-05  1.16050008e-05 -1.95957270e-05 -9.41006054e-06\n",
      "  6.33019608e-05  2.60629850e-05 -2.89793508e-05  4.01625786e-04\n",
      " -2.32424768e-04 -9.21984658e-03  2.88124053e-06  3.64696756e-06\n",
      "  4.58363781e-06  2.15366135e-06  7.15046019e-06  1.13651851e-06\n",
      "  2.90104164e-03  6.04360529e-03  7.21253060e-03 -5.64548242e-04\n",
      "  4.66452020e-05  7.87066330e-06 -1.76779712e-05 -2.38924078e-06\n",
      "  8.21948237e-05  2.81006208e-05  4.20165590e-04]\n"
     ]
    }
   ],
   "source": [
    "def sgd_for_svm_demo(y, X):\n",
    "    \n",
    "    max_iter = 2 * int(1e5)\n",
    "    gamma = 1e-4\n",
    "    lambda_ = int(1e4)   # big because scales with N due to the formulation of the problem (not an averaged loss)\n",
    "    \n",
    "    num_examples, num_features = X.shape\n",
    "    w = np.zeros(num_features)\n",
    "    \n",
    "    for it in range(max_iter):\n",
    "        # n = sample one data point uniformly at random data from x\n",
    "        n = random.randint(0,num_examples-1)\n",
    "        \n",
    "        grad = calculate_stochastic_gradient(y, X, w, lambda_, n, num_examples)\n",
    "        w -= gamma/(it+1) * grad\n",
    "        \n",
    "        if it % 10000 == 0:\n",
    "            cost = calculate_primal_objective(y, X, w, lambda_)\n",
    "            print(\"iteration={i}, cost={c}\".format(i=it, c=cost))\n",
    "    \n",
    "    print(\"training accuracy = {l}\".format(l=calculate_accuracy(y, X, w)))\n",
    "    print(w)\n",
    "\n",
    "sgd_for_svm_demo(y, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Descent (Ascent) for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the closed-form update for the n-th variable alpha, in the dual optimization problem, given alpha and the current corresponding w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coordinate_update(y, X, lambda_, alpha, w, n):\n",
    "    \"\"\"compute a coordinate update (closed form) for coordinate n.\n",
    "    X: the dataset matrix, shape = (num_examples, num_features)\n",
    "    y: the corresponding +1 or -1 labels, shape = (num_examples)\n",
    "    w: shape = (num_features)\n",
    "    n: the coordinate to be updated\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    # calculate the update of coordinate at index=n.\n",
    "    x_n, y_n = X[n], y[n]\n",
    "    old_alpha_n = np.copy(alpha[n])\n",
    "    \n",
    "    alpha[n] = lambda_ / ((x_n * x_n).sum() * y_n ** 2)\n",
    "    if alpha[n] > 1 :\n",
    "        alpha[n] = 1\n",
    "    elif alpha[n] < 0: \n",
    "        alpha[n] = 0\n",
    "    \n",
    "    \n",
    "    #update w\n",
    "    w = (X.T * y) @ alpha / lambda_\n",
    "    return w, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dual_objective(y, X, w, alpha, lambda_):\n",
    "    \"\"\"calculate the objective for the dual problem.\"\"\"\n",
    "    k = X @ X.T\n",
    "    yxxy = y * k * y\n",
    "    return alpha.sum() - (1/(2 * lambda_)) * alpha.T @ yxxy @ alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0, primal:751.82934, dual:0.00905, gap:751.82029\n",
      "iteration=10000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=20000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=30000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=40000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=50000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=60000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=70000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=80000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=90000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=100000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=110000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=120000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=130000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=140000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=150000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=160000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=170000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=180000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "iteration=190000, primal:144382.74005, dual:-2231.44510, gap:146614.18515\n",
      "training accuracy = 0.6274165202108963\n",
      "[0.0019408  0.00177478 0.00226298 0.02043407 0.0023727  0.01254617\n",
      " 0.00270011 0.00859649 0.01193145 0.01329157 0.0050281  0.00428926\n",
      " 0.00323431 0.00710534 0.01184635 0.00739576 0.00558098 0.00416185\n",
      " 0.00135043 0.01182536 0.0145823  0.0546707  0.00672196 0.00112669\n",
      " 0.00172982 0.00331323 0.00801791 0.00311481 0.00457857 0.00407458\n",
      " 0.0024828  0.0099255  0.00376894 0.00213361 0.00439808 0.00434696\n",
      " 0.00939146 0.01701416 0.00992136 0.01133651 0.01043918 0.0239824\n",
      " 0.00249607 0.00874634 0.01174294 0.00273372 0.09401935 0.01133031\n",
      " 0.01795851 0.01215462 0.02152415 0.01287882 0.02067922 0.00350099\n",
      " 0.00626864 0.02286744 0.00167069 0.00699885 0.01460398 0.07511802\n",
      " 0.04095699 0.07533389 0.00731714 0.05768666 0.00942598 0.00752722\n",
      " 0.05211314 0.02591287 0.05641793 0.01736932 0.00207768 0.06707697\n",
      " 0.00267881 0.00977316 0.0163649  0.004546   0.01434428 0.00273166\n",
      " 0.00235296 0.01514145 0.02212786 0.01499909 0.00098206 0.00332232\n",
      " 0.0187226  0.00265142 0.00910388 0.00260688 0.01771617 0.00913427\n",
      " 0.00902786 0.00803322 0.00985355 0.0122492  0.00633879 0.00212208\n",
      " 0.0215895  0.04327737 0.02240545 0.00893488 0.00843852 0.16631618\n",
      " 0.01902393 0.04343919 0.03484728 0.01013489 0.02187112 0.01890597\n",
      " 0.00125877 0.02273651 0.04358237 0.0190312  0.01083737 0.03796281\n",
      " 0.07024335 0.01825229 0.07067446 0.0059031  0.00441664 0.00386244\n",
      " 0.02291308 0.00277379 0.00133059 0.01013022 0.0138279  0.01119834\n",
      " 0.00933334 0.00269819 0.00943103 0.00250527 0.01920642 0.00518903\n",
      " 0.00483435 0.00686286 0.00268357 0.01426383 0.02025251 0.02589474\n",
      " 0.00694758 0.02865248 0.04768895 0.00451248 0.02350469 0.01439534\n",
      " 0.03004458 0.02164553 0.01807199 0.00865706 0.00980365 0.01126023\n",
      " 0.01497183 0.08947774 0.04042725 0.02841416 0.01302869 0.0183354\n",
      " 0.00375092 0.00534768 0.02003648 0.02722014 0.02046434 0.00275723\n",
      " 0.00153495 0.01859765 0.00115124 0.0090134  0.03225253 0.00415206\n",
      " 0.0026454  0.00890858 0.01880685 0.00750999 0.00559727 0.03253581\n",
      " 0.03331336 0.07938716 0.03919543 0.0059295  0.01505693 0.01666771\n",
      " 0.00064487 0.00162766 0.00448667 0.02545241 0.00677124 0.03298672\n",
      " 0.00321761 0.02161221 0.02287917 0.01890618 0.01023705 0.01633447\n",
      " 0.05425364 0.01190603 0.00914814 0.01553255 0.00977353 0.00385406\n",
      " 0.00237158 0.0065442  0.01569782 0.00403745 0.00145988 0.00582294\n",
      " 0.01408776 0.00658686 0.04833737 0.00431552 0.01411608 0.00704124\n",
      " 0.00223994 0.0201536  0.00077881 0.00506517 0.00946739 0.01088232\n",
      " 0.02065109 0.03600594 0.00178168 0.00129929 0.01188779 0.01240865\n",
      " 0.04003006 0.00557817 0.01219905 0.00834959 0.03625418 0.00872771\n",
      " 0.01531649 0.01294588 0.00444817 0.02667391 0.02591092 0.00188794\n",
      " 0.05001567 0.01116016 0.00086582 0.00207426 0.01004221 0.00348487\n",
      " 0.01225107 0.01863589 0.02564057 0.0115306  0.00295371 0.03469487\n",
      " 0.01487563 0.01448855 0.02930847 0.02379469 0.00167542 0.02277303\n",
      " 0.00190685 0.00418849 0.00188501 0.00936563 0.00191183 0.0071358\n",
      " 0.00463898 0.00603316 0.00195721 0.00418909 0.00382013 0.00633442\n",
      " 0.00337813 0.00071937 0.03192202 0.01295423 0.01573245 0.0333545\n",
      " 0.01127158 0.02687587 0.00125097 0.04549705 0.00360179 0.02334295\n",
      " 0.02636219 0.00360171 0.01119896 0.01229958 0.0023167  0.0207297\n",
      " 0.00249641 0.00574226 0.01565643 0.01745978 0.01963641 0.01627723\n",
      " 0.02772003 0.02625327 0.00962075 0.00868328 0.01605893 0.02129276\n",
      " 0.01772122 0.0125881  0.03368469 0.01965351 0.00913993 0.03930756\n",
      " 0.00173557 0.01843771 0.00222501 0.03795843 0.02402287 0.02406905\n",
      " 0.01406157 0.06669136 0.01209925 0.01331013 0.02368175 0.00854391\n",
      " 0.01529692 0.02506946 0.08768306 0.0184274  0.02041547 0.00303206\n",
      " 0.06326756 0.01965284 0.03724414 0.00226452 0.01584891 0.00184891\n",
      " 0.01768478 0.01707026 0.01039807 0.02053711 0.00512558 0.00600913\n",
      " 0.00554582 0.01459458 0.02832882 0.02451511 0.01898174 0.00364725\n",
      " 0.01633878 0.00210435 0.03922675 0.00086267 0.00848974 0.04678077\n",
      " 0.0289119  0.00260001 0.02185703 0.04139737 0.01887051 0.00801978\n",
      " 0.02491514 0.02258768 0.02059108 0.00692944 0.00068456 0.00607282\n",
      " 0.02728082 0.01810752 0.01588336 0.01171455 0.06312004 0.03541998\n",
      " 0.01664514 0.01424171 0.01670107 0.0057198  0.01310946 0.00204433\n",
      " 0.00228722 0.01621265 0.00080769 0.00138218 0.00477025 0.00833839\n",
      " 0.00231292 0.00177841 0.01243961 0.00719732 0.03995597 0.01252083\n",
      " 0.01270199 0.02471299 0.02566678 0.02847218 0.02199571 0.01689136\n",
      " 0.01427869 0.00962389 0.01963002 0.01074371 0.02707731 0.00317838\n",
      " 0.03710355 0.0608388  0.00410073 0.00156993 0.01894503 0.01164933\n",
      " 0.01271486 0.01600454 0.02485673 0.01933133 0.00366299 0.01791494\n",
      " 0.01469116 0.01635727 0.0192602  0.02675105 0.00639188 0.01428036\n",
      " 0.00352044 0.01650598 0.02241124 0.02671217 0.05802056 0.00795197\n",
      " 0.0070846  0.02098324 0.04688286 0.00265324 0.01718818 0.02719987\n",
      " 0.02204752 0.00899743 0.02459905 0.01171694 0.04148637 0.04055642\n",
      " 0.03146525 0.02610417 0.029931   0.01634675 0.00839545 0.01996178\n",
      " 0.00263586 0.00259656 0.00926252 0.00878319 0.01492012 0.01045356\n",
      " 0.01071969 0.01161195 0.0263247  0.00392521 0.01160359 0.03832084\n",
      " 0.00372615 0.0210866  0.00325903 0.00857968 0.00871309 0.00165001\n",
      " 0.0217992  0.00287425 0.02057849 0.0100161  0.01514013 0.01206164\n",
      " 0.02140834 0.01412811 0.01463671 0.04587462 0.00275176 0.00040408\n",
      " 0.01022187 0.02330345 0.01283337 0.01168792 0.01277266 0.04221109\n",
      " 0.00325275 0.02165964 0.04123624 0.0184374  0.00759173 0.01831055\n",
      " 0.03040968 0.01560974 0.00915294 0.01116534 0.0252533  0.00633666\n",
      " 0.01913281 0.00930808 0.01329701 0.0122546  0.0076195  0.01715948\n",
      " 0.00865016 0.00225407 0.02023865 0.00515331 0.01606303 0.00402985\n",
      " 0.00323407 0.01895775 0.01371854 0.009092   0.0152711  0.01623936\n",
      " 0.00266833 0.00204345 0.00810532 0.01000529 0.01827848 0.00093707\n",
      " 0.06189663 0.05211962 0.02058181 0.03171303 0.00641765 0.00719459\n",
      " 0.02384477 0.00941636 0.00955552 0.00833273 0.0068807  0.02530682\n",
      " 0.00296117 0.00236323 0.01393001 0.01536418 0.05481578 0.00095589\n",
      " 0.02832867 0.01192472 0.04193794 0.07539194 0.01188331 0.01807061\n",
      " 0.01250298 0.0194058  0.02000635 0.02035031 0.01053978 0.00224875\n",
      " 0.03205665 0.00198498 0.01053505 0.02388637 0.09884563 0.11590038\n",
      " 0.02597337 0.00900991 0.00866999 0.01418636 0.01196093 0.0113497\n",
      " 0.03785327 0.04148067 0.04457318 0.02492896 0.03206192 0.02882954\n",
      " 0.01590117 0.06015098 0.01570295 0.04119463 0.04386229 0.05158547\n",
      " 0.0100765  0.02484198 0.01134473 0.02808754 0.00723455 0.00192589\n",
      " 0.00156949 0.0021548  0.00492016 0.00200765 0.08868872]\n"
     ]
    }
   ],
   "source": [
    "def coordinate_descent_for_svm_demo(y, X):\n",
    "    max_iter = 2*int(1e5)\n",
    "    lambda_ = int(1e4)   # use same lambda as before in order to compare\n",
    "\n",
    "    num_examples, num_features = X.shape\n",
    "    w = np.zeros(num_features)\n",
    "    alpha = np.zeros(num_examples)\n",
    "    \n",
    "    for it in range(max_iter):\n",
    "        # n = sample one data point uniformly at random data from x\n",
    "        n = random.randint(0,num_examples-1)\n",
    "        \n",
    "        w, alpha = calculate_coordinate_update(y, X, lambda_, alpha, w, n)\n",
    "            \n",
    "        if it % 10000 == 0:\n",
    "            # primal objective\n",
    "            primal_value = calculate_primal_objective(y, X, w, lambda_)\n",
    "            # dual objective\n",
    "            dual_value = calculate_dual_objective(y, X, w, alpha, lambda_)\n",
    "            # primal dual gap\n",
    "            duality_gap = primal_value - dual_value\n",
    "            print('iteration=%i, primal:%.5f, dual:%.5f, gap:%.5f'%(\n",
    "                    it, primal_value, dual_value, duality_gap))\n",
    "    print(\"training accuracy = {l}\".format(l=calculate_accuracy(y, X, w)))\n",
    "    print(alpha)\n",
    "\n",
    "coordinate_descent_for_svm_demo(y, X)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
